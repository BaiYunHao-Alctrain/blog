```python


```




    0.83



1.决策树
    1.1.分类树
    1.2 回归树
2.集成学习
    2.1bagging：随机森林
    2.2boosting:adaboost,GBDT,XGBoost,LightGBM
一、决策树
1.1.决策树的直观理解
		根节点-叶子节点
1.2分类树
1.2.1信息熵
	信息熵是用来衡量信息不确定性的指标，不确定性是一个事件出现不同结果的可能性。计算方法如下所示：
   ![](https://ai-studio-static-online.cdn.bcebos.com/6c6a723642c64e4f8cc2bc72ea6109f04efb3d6ca35346be9d94f0845023913b)
   用H(x)来表示，越小说明不确定性越低，越好。越大说明不确定性越高！
 条件熵：在给定随即变量Y的条件下，随机变量X的不确定性。
 ![](https://ai-studio-static-online.cdn.bcebos.com/774b2eb2a1824a8da4d3c651eefef635b745ad9d29954d99897903243465147e)
 
 信息增益：信息熵-条件熵，代表了在一个条件下，信息不确定性减少的程度。
 ![](https://ai-studio-static-online.cdn.bcebos.com/93e7c879b8e54e11bf24a61d6a4059bcf6c6a9cb181d4da7a8277bfe70d48284)
 信息增益越大，划分结果越好。（即使H（X）减少的最快）


1.2.2基尼系数
基尼指数（Gini不纯度）表示在样本集合中一个随机选中的样本被分错的概率。
注意：Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之则集合越不纯。当集合中的所有样本为一个类时，基尼指数为0.
基尼指数的计算方法：
![](https://ai-studio-static-online.cdn.bcebos.com/822a60632f994c7ebc3f02dbf920fec547eb19de1da94f03a2cef2be93c9ee5e)
其中，pk表示选中的样本属于第k个类别的概率。
同理，也有基尼增益。


```python

```

1.3回归树
用树模型做回归问题，每一片叶子都输出一个预测值。预测值一般是叶子节点所含训练集元素输出的均值。
分支标准：
标准方差。回归树使用某一特征把原集合分为多个子集，用标准方差衡量子集中的元素是否相近，越小表示越相近。
首先计算根节点的标准方差.然后计算变化系数，变化系数用于决定是否停止进一步分叉。
![](https://ai-studio-static-online.cdn.bcebos.com/08fc7b06aaea4cb9bba1410793057c53daa9e93851584f08a31b0474020fa7f6)
使用标准方差降低最多的特征来进行分支。

2.1集成学习简介
集成学习是通过构建并且组合多个学习器来完成学习任务的算法。集成学习常用的有两类：
Bagging：基学习器之间无强烈的依赖关系，可以同时生成的并行化方法。（随机森林）
Boosting：基学习器之间存在强烈的依赖关系，必须串行生成基分类器的办法。（Adaboost，gbdt，xgboost，Light GBM）

Bagging方法：
有一群训练数据，生成n个弱学习器，每次都从原始数据中随机采样为i样本，将i样本送入第i个弱学习器学习。所以一共生成了n个集成学习器。
拿到了测试数据后，n个集成学习器一起对测试集进行预测，得到n个结果，取结果中的众数作为最终结果。
![](https://ai-studio-static-online.cdn.bcebos.com/4d9ff75db9c14eb6a2febb59b14aa44bf4c73571f5fa43eea20359f39d1d803f)


Boosting方法：
把“弱学习算法”提升为“强学习算法”的过程。通过反复学习得到一系列的弱分类器（决策树和逻辑回归），组合这些弱分类器得到一个强分类器。Boosting算法要涉及到两个部分，加法模型（是弱分类的相加）和前向分步算法（下一个弱分类器是在上一个弱分类器的基础上生成的）。![](https://ai-studio-static-online.cdn.bcebos.com/3736647c1564401ab8fa5e5ad3845ca06b5ab6ea10d849deb7e5ae35f82dca65)

加法模型：
就是说强分类器是由一系列的弱分类器线性相加而成。一般的组合形式如下：
![](https://ai-studio-static-online.cdn.bcebos.com/0ad3875269d1450393d385ced98979b383ca149aa0f343e2859c11f5b7627fa5)
Fm是强分类器的集成结果，h是弱分类器的结果，βm是每一个弱分类器的比重。

前向分步模型：
在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。即：
![](https://ai-studio-static-online.cdn.bcebos.com/447373c70cfa45ea9697a06804f48e21e42037d8a438406a9947233a526e897b)




```python

```


```python

```

2.2随机森林
随机森林=bagging+决策树
同时训练多个决策树，预测时综合考虑多个结果进行预测，例如取各个节点的均值（回归），或者是众数（分类）。
优势：避免过拟合。
随机森林的随机性体现在两点：
1.从原来的训练数据集随机（带放回bootstrap）取一个子集作为森林中某一个决策树的训练数据集。
2.每一次选择分叉的特征时，限定为在随机选择的特征的子集中寻找一个特征。![](https://ai-studio-static-online.cdn.bcebos.com/3a09c649cbd9488ab15efe23b5906a05abb223dad8814f56856e473b73259cec)



```python

```

2.3Adaboost
Adaboost是把关注点放在被错误分类的样本上，减少上一轮被正确分类的样本权值，提高被错误分类的样本权值。
Adaboost采用加权投票的方法，分类误差小的弱分类器权重打，而分类误差大的弱分类器的权重小。
![](https://ai-studio-static-online.cdn.bcebos.com/5f6f92f6867e49b0bf7d60607831b78ed1ecfcf178294fd993a6772d3ad9c69a)

![](https://ai-studio-static-online.cdn.bcebos.com/f1fc690d7de84a4d871aac5201c46dad398b570494214555a9c33e3a3ee530f6)
![](https://ai-studio-static-online.cdn.bcebos.com/5caa3333529443edb5cb66b278a6e5c54ffd7ed814e94ee4bce66ebd43ee18bf)


2.4GBDT
2.4.1BDT(提升树）
Boosting Decision Tree。
![](https://ai-studio-static-online.cdn.bcebos.com/147655b654f14c419781316c3c26a08d38581e81e9554a04a0903233f4080c1c)
提升树的本质是加法模型和前向分布算法：
f0(x)=0
![](https://ai-studio-static-online.cdn.bcebos.com/89c5c1dd76b34774845a6e1cd382483f48602905b0cd4ea593139f019005e19e)
![](https://ai-studio-static-online.cdn.bcebos.com/fb4b4112974a4e429332e439ec5aed9198226ccae80f49a7ad94e856d67e733c)

2.4.2GBDT

梯度提升决策树，梯度提升+决策树。利用损失函数的负梯度来拟合基学习器
![](https://ai-studio-static-online.cdn.bcebos.com/cced2f7451864cdd9de2aca707deaabccc1f20adc33744729391b2edd52f13d3)
因为负梯度可以用来近似为残差。
GBDT是使用梯度提升的回归树（CART），CART树把整个空间划分为n个空间。


